{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAONrXZUWa-L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "i615IbQL0cU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:\n",
        "What is Information Gain, and how is it used in Decision Trees?\n",
        "Answer:\n",
        "Information Gain measures the reduction in entropy or impurity after a dataset is split on a particular feature. It quantifies how much information a feature gives about the class.\n",
        "In Decision Trees, Information Gain is used to select the best feature at each node — the one that provides the highest reduction in impurity. The higher the Information Gain, the better the feature for splitting the data.\n",
        "\n",
        "Question 2:\n",
        "What is the difference between Gini Impurity and Entropy?\n",
        "Answer:\n",
        "Measure\t\tRange\tInterpretation\tWhen to Use\n",
        "Gini Impurity\t\t0 to 0.5\tMeasures the probability of incorrectly classifying a randomly chosen element\tFaster to compute, often used in CART\n",
        "Entropy\t\t0 to 1\tMeasures the amount of disorder or uncertainty\tUsed in ID3, C4.5 decision trees\n",
        "Summary:\n",
        "Both measure impurity, but Gini Impurity is computationally simpler, while Entropy provides more theoretical grounding based on information theory.\n",
        "\n",
        "Question 3:\n",
        "What is Pre-Pruning in Decision Trees?\n",
        "Answer:\n",
        "Pre-Pruning (also known as early stopping) is a technique to stop the tree from growing once splitting no longer significantly improves the model.\n",
        "It helps to prevent overfitting by using conditions such as:\n",
        "•\tMinimum number of samples required to split a node.\n",
        "•\tMaximum depth of the tree.\n",
        "•\tMinimum information gain threshold.\n",
        "This ensures the model remains simpler and more generalizable.\n",
        "\n",
        "\n",
        "Question 5:\n",
        "What is a Support Vector Machine (SVM)?\n",
        "Answer:\n",
        "Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression.\n",
        "It finds the optimal hyperplane that maximizes the margin between different classes in the feature space.\n",
        "SVMs are effective in high-dimensional spaces and work well even when the number of features exceeds the number of samples.\n",
        "\n",
        "Question 6:\n",
        "What is the Kernel Trick in SVM?\n",
        "Answer:\n",
        "The Kernel Trick allows SVM to perform classification in a higher-dimensional space without explicitly transforming the data.\n",
        "It uses kernel functions (like linear, polynomial, or RBF) to compute the inner products between the images of data points in that higher-dimensional space, enabling non-linear decision boundaries.\n",
        "Common kernels:\n",
        "•\tLinear\n",
        "•\tPolynomial\n",
        "•\tRadial Basis Function (RBF)\n",
        "\n",
        "Question 8:\n",
        "What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "Answer:\n",
        "Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem, which predicts class membership probabilities.\n",
        "It assumes that all features are independent of each other given the class label — an assumption that’s rarely true in practice.\n",
        "Because of this unrealistic independence assumption, it is called “Naïve.”\n",
        "Despite this, it performs well in many applications like text classification and spam filtering.\n",
        "\n",
        "Question 9:\n",
        "Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
        "Answer:\n",
        "Type\tData Type\tUse Case\tDescription\n",
        "GaussianNB\tContinuous\tNumeric features\tAssumes features follow a normal (Gaussian) distribution.\n",
        "MultinomialNB\tDiscrete counts\tText classification, document term counts\tWorks with word frequencies or occurrence counts.\n",
        "BernoulliNB\tBinary\tBinary/Boolean features\tSuitable for data where features are 0 or 1 (e.g., presence/absence).\n"
      ],
      "metadata": {
        "id": "wuQbcH9A0cYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Question 4:\n",
        "# Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGm6S_SDWqdu",
        "outputId": "8e29400d-f2b9-44d0-b768-3a031f411ee6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7:\n",
        "# Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Linear SVM\n",
        "svc_linear = SVC(kernel='linear', random_state=42)\n",
        "svc_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svc_linear.predict(X_test)\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train RBF SVM\n",
        "svc_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svc_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svc_rbf.predict(X_test)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", acc_linear)\n",
        "print(\"RBF Kernel Accuracy:\", acc_rbf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noq7JEDOZPZq",
        "outputId": "3b0d2acd-65ec-427e-c694-6167dedeedbb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 1.0\n",
            "RBF Kernel Accuracy: 0.8055555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 10:\n",
        "# Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gaussian Naive Bayes model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Gaussian Naive Bayes Accuracy:\", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tijwbnyJ0NiW",
        "outputId": "0db6b99f-4a88-4823-accf-a977a3a3be7c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naive Bayes Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    }
  ]
}